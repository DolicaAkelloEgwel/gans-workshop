{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANs with Python\n",
    "\n",
    "## Setup\n",
    "\n",
    "### 1. Go to this link: bit.ly/45svhk0\n",
    "### 2. Log in to your Google Drive account\n",
    "### 3. Make sure your files are in the right place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ggJNE19puADT"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import pylab as pl\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "do_preprocess = True\n",
    "from_checkpoint = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `do_preprocess` - Determines if we run or skip the code for resizing our image files. This will need to be done at least once.\n",
    "- `from_checkpoint` - Determines if the GAN starts from scratch, or picks up where it left off by loading a model file. For the first run this will have to be set to False."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pGm8QAAeqPNi",
    "outputId": "53e7a301-72d9-415d-8477-d4c16b891783"
   },
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google knows to use the T4 Runtime when I run GPU code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Folder Names\n",
    "\n",
    "- One folder for the resized images that we will feed into the GAN\n",
    "- Another folder for the model files that as as snapshots of how far the GAN has come along\n",
    "- And another for the computer-generated images\n",
    "\n",
    "The `try-except` is there so that this code can work on Colab or on a desktop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uHf6TJ9t_f-c",
    "outputId": "960210a4-56fa-4922-ddb1-d417b2caf279"
   },
   "outputs": [],
   "source": [
    "GANS_WORKSHOP_FOLDER = \"gans-workshop-files\"\n",
    "data_folder_name = \"pokemon\"\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount(\"/content/drive\")\n",
    "    project_dir = \"./drive/MyDrive/\"\n",
    "except ModuleNotFoundError:\n",
    "    project_dir = os.getcwd()\n",
    "\n",
    "workshop_dir = os.path.join(project_dir, GANS_WORKSHOP_FOLDER)\n",
    "data_dir = os.path.join(workshop_dir, data_folder_name)\n",
    "data_resized_dir = os.path.join(workshop_dir, f\"{data_folder_name}-resized\")\n",
    "models_folder = os.path.join(workshop_dir, f\"{data_folder_name}-models\")\n",
    "image_folder = os.path.join(workshop_dir, f\"{data_folder_name}-gans-images\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the Files\n",
    "\n",
    "This code is designed to work with 128x128 images, so we're going to resize the images and place them in a new folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9mIQnG0tqP8y"
   },
   "outputs": [],
   "source": [
    "if do_preprocess:\n",
    "    # Make a folder for the resized images if one doesn't already exist\n",
    "    if not os.path.isdir(data_resized_dir):\n",
    "        os.mkdir(data_resized_dir)\n",
    "\n",
    "    # Go through each of our input images, resize them, and then save them to the new folder\n",
    "    for image_filename in os.listdir(data_dir):\n",
    "        try:\n",
    "            image = cv2.imread(os.path.join(data_dir, image_filename))\n",
    "            image = cv2.resize(image, (128, 128))\n",
    "            cv2.imwrite(os.path.join(data_resized_dir, image_filename), image)\n",
    "        except Exception as e:\n",
    "            print(str(e))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Helper Functions\n",
    "\n",
    "These are some simple functions for getting image data. This helps with keeping the code modular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQGHb9DPqeBY"
   },
   "outputs": [],
   "source": [
    "# This part was taken from Udacity Face generator project\n",
    "def get_image(image_path, mode):\n",
    "    \"\"\"Creates a numpy array from an image path.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): The path of the image to lad.\n",
    "        mode (_type_): The mode argument for the convert method.\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    return np.array(image.convert(mode))\n",
    "\n",
    "\n",
    "def get_batch(image_files, mode):\n",
    "    \"\"\"Gets a batch of images\n",
    "\n",
    "    Args:\n",
    "        image_files (_type_): _description_\n",
    "        mode (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"\n",
    "    data_batch = np.array(\n",
    "        [get_image(sample_file, mode) for sample_file in image_files]\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    # Make sure the images are in 4 dimensions\n",
    "    if len(data_batch.shape) < 4:\n",
    "        data_batch = data_batch.reshape(data_batch.shape + (1,))\n",
    "\n",
    "    return data_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a function that will plot several images in a grid. We'll use this to monitor the progress of the GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UpNvZp97rZwB"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def images_square_grid(images, mode):\n",
    "    \"\"\"\n",
    "    Save images as a square grid\n",
    "    :param images: Images to be used for the grid\n",
    "    :param mode: The mode to use for images\n",
    "    :return: Image of images in a square grid\n",
    "    \"\"\"\n",
    "    # Get maximum size for square grid of images\n",
    "    save_size = math.floor(np.sqrt(images.shape[0]))\n",
    "\n",
    "    # Scale to 0-255\n",
    "    images = (((images - images.min()) * 255) / (images.max() - images.min())).astype(\n",
    "        np.uint8\n",
    "    )\n",
    "\n",
    "    # Put images in a square arrangement\n",
    "    images_in_square = np.reshape(\n",
    "        images[: save_size * save_size],\n",
    "        (save_size, save_size, images.shape[1], images.shape[2], images.shape[3]),\n",
    "    )\n",
    "    if mode == \"L\":\n",
    "        images_in_square = np.squeeze(images_in_square, 4)\n",
    "\n",
    "    # Combine images to grid image\n",
    "    new_im = Image.new(mode, (images.shape[1] * save_size, images.shape[2] * save_size))\n",
    "    for col_i, col_images in enumerate(images_in_square):\n",
    "        for image_i, image in enumerate(col_images):\n",
    "            im = Image.fromarray(image, mode)\n",
    "            new_im.paste(im, (col_i * images.shape[1], image_i * images.shape[2]))\n",
    "\n",
    "    return new_im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a `Dataset` object for Tensorflow to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    \"\"\"\n",
    "    Dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_files):\n",
    "        \"\"\"\n",
    "        Initalize the class\n",
    "        :param dataset_name: Database name\n",
    "        :param data_files: List of files in the database\n",
    "        \"\"\"\n",
    "        IMAGE_WIDTH = 128\n",
    "        IMAGE_HEIGHT = 128\n",
    "\n",
    "        self.image_mode = \"RGB\"\n",
    "        image_channels = 3\n",
    "\n",
    "        self.data_files = data_files\n",
    "        self.shape = len(data_files), IMAGE_WIDTH, IMAGE_HEIGHT, image_channels\n",
    "\n",
    "    def get_batches(self, batch_size):\n",
    "        \"\"\"\n",
    "        Generate batches\n",
    "        :param batch_size: Batch Size\n",
    "        :return: Batches of data\n",
    "        \"\"\"\n",
    "        IMAGE_MAX_VALUE = 255\n",
    "\n",
    "        current_index = 0\n",
    "        while current_index + batch_size <= self.shape[0]:\n",
    "            data_batch = get_batch(\n",
    "                self.data_files[current_index : current_index + batch_size],\n",
    "                self.image_mode,\n",
    "            )\n",
    "\n",
    "            current_index += batch_size\n",
    "\n",
    "            yield data_batch / IMAGE_MAX_VALUE - 0.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will display 9 of our resized images by using the `images_square_grid` function that was defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "id": "pUkW_Ylyrkke",
    "outputId": "0e6a79de-3f40-406a-8861-12b0400b5520"
   },
   "outputs": [],
   "source": [
    "resized_data_filenames = [\n",
    "    data_resized_dir + \"/\" + i for i in os.listdir(data_resized_dir)\n",
    "]\n",
    "show_n_images = 9\n",
    "train_images = get_batch(resized_data_filenames[:show_n_images], \"RGB\")\n",
    "plt.imshow(images_square_grid(train_images, \"RGB\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Tensorflow `placeholder` type for defining the model inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zVPRWWhIDUdi"
   },
   "outputs": [],
   "source": [
    "def model_inputs(real_dim, z_dim):\n",
    "    \"\"\"\n",
    "    Create the model inputs\n",
    "    :param real_dim: tuple containing width, height and channels\n",
    "    :param z_dim: The dimension of Z\n",
    "    :return: Tuple of (tensor of real input images, tensor of z data, learning rate G, learning rate D)\n",
    "    \"\"\"\n",
    "    inputs_real = tf.compat.v1.placeholder(\n",
    "        tf.float32, (None, *real_dim), name=\"inputs_real\"\n",
    "    )\n",
    "    inputs_z = tf.compat.v1.placeholder(tf.float32, (None, z_dim), name=\"input_z\")\n",
    "    learning_rate_G = tf.compat.v1.placeholder(tf.float32, name=\"learning_rate_G\")\n",
    "    learning_rate_D = tf.compat.v1.placeholder(tf.float32, name=\"learning_rate_D\")\n",
    "\n",
    "    return inputs_real, inputs_z, learning_rate_G, learning_rate_D"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_OrZ8idXDYfy"
   },
   "outputs": [],
   "source": [
    "def generator(z, output_channel_dim, is_train=True):\n",
    "    \"\"\"Build the generator network.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    z : Input tensor for the generator\n",
    "    output_channel_dim : Shape of the generator output\n",
    "    n_units : Number of units in hidden layer\n",
    "    reuse : Reuse the variables with tf.variable_scope\n",
    "    alpha : leak parameter for leaky ReLU\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out:\n",
    "    \"\"\"\n",
    "    with tf.compat.v1.variable_scope(\"generator\", reuse=not is_train):\n",
    "        # First FC layer --> 8x8x1024\n",
    "        fc1 = tf.compat.v1.layers.dense(z, 8 * 8 * 1024)\n",
    "\n",
    "        # Reshape it\n",
    "        fc1 = tf.reshape(fc1, (-1, 8, 8, 1024))\n",
    "\n",
    "        # Leaky ReLU\n",
    "        fc1 = tf.nn.leaky_relu(fc1, alpha=alpha)\n",
    "\n",
    "        # Transposed conv 1 --> BatchNorm --> LeakyReLU\n",
    "        # 8x8x1024 --> 16x16x512\n",
    "        trans_conv1 = tf.compat.v1.layers.conv2d_transpose(\n",
    "            inputs=fc1,\n",
    "            filters=512,\n",
    "            kernel_size=[5, 5],\n",
    "            strides=[2, 2],\n",
    "            padding=\"SAME\",\n",
    "            kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02),\n",
    "            name=\"trans_conv1\",\n",
    "        )\n",
    "\n",
    "        batch_trans_conv1 = tf.compat.v1.layers.batch_normalization(\n",
    "            inputs=trans_conv1,\n",
    "            training=is_train,\n",
    "            epsilon=1e-5,\n",
    "            name=\"batch_trans_conv1\",\n",
    "        )\n",
    "\n",
    "        trans_conv1_out = tf.nn.leaky_relu(\n",
    "            batch_trans_conv1, alpha=alpha, name=\"trans_conv1_out\"\n",
    "        )\n",
    "\n",
    "        # Transposed conv 2 --> BatchNorm --> LeakyReLU\n",
    "        # 16x16x512 --> 32x32x256\n",
    "        trans_conv2 = tf.compat.v1.layers.conv2d_transpose(\n",
    "            inputs=trans_conv1_out,\n",
    "            filters=256,\n",
    "            kernel_size=[5, 5],\n",
    "            strides=[2, 2],\n",
    "            padding=\"SAME\",\n",
    "            kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02),\n",
    "            name=\"trans_conv2\",\n",
    "        )\n",
    "\n",
    "        batch_trans_conv2 = tf.compat.v1.layers.batch_normalization(\n",
    "            inputs=trans_conv2,\n",
    "            training=is_train,\n",
    "            epsilon=1e-5,\n",
    "            name=\"batch_trans_conv2\",\n",
    "        )\n",
    "\n",
    "        trans_conv2_out = tf.nn.leaky_relu(\n",
    "            batch_trans_conv2, alpha=alpha, name=\"trans_conv2_out\"\n",
    "        )\n",
    "\n",
    "        # Transposed conv 3 --> BatchNorm --> LeakyReLU\n",
    "        # 32x32x256 --> 64x64x128\n",
    "        trans_conv3 = tf.compat.v1.layers.conv2d_transpose(\n",
    "            inputs=trans_conv2_out,\n",
    "            filters=128,\n",
    "            kernel_size=[5, 5],\n",
    "            strides=[2, 2],\n",
    "            padding=\"SAME\",\n",
    "            kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02),\n",
    "            name=\"trans_conv3\",\n",
    "        )\n",
    "\n",
    "        batch_trans_conv3 = tf.compat.v1.layers.batch_normalization(\n",
    "            inputs=trans_conv3,\n",
    "            training=is_train,\n",
    "            epsilon=1e-5,\n",
    "            name=\"batch_trans_conv3\",\n",
    "        )\n",
    "\n",
    "        trans_conv3_out = tf.nn.leaky_relu(\n",
    "            batch_trans_conv3, alpha=alpha, name=\"trans_conv3_out\"\n",
    "        )\n",
    "\n",
    "        # Transposed conv 4 --> BatchNorm --> LeakyReLU\n",
    "        # 64x64x128 --> 128x128x64\n",
    "        trans_conv4 = tf.compat.v1.layers.conv2d_transpose(\n",
    "            inputs=trans_conv3_out,\n",
    "            filters=64,\n",
    "            kernel_size=[5, 5],\n",
    "            strides=[2, 2],\n",
    "            padding=\"SAME\",\n",
    "            kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02),\n",
    "            name=\"trans_conv4\",\n",
    "        )\n",
    "\n",
    "        batch_trans_conv4 = tf.compat.v1.layers.batch_normalization(\n",
    "            inputs=trans_conv4,\n",
    "            training=is_train,\n",
    "            epsilon=1e-5,\n",
    "            name=\"batch_trans_conv4\",\n",
    "        )\n",
    "\n",
    "        trans_conv4_out = tf.nn.leaky_relu(\n",
    "            batch_trans_conv4, alpha=alpha, name=\"trans_conv4_out\"\n",
    "        )\n",
    "\n",
    "        # Transposed conv 5 --> tanh\n",
    "        # 128x128x64 --> 128x128x3\n",
    "        logits = tf.compat.v1.layers.conv2d_transpose(\n",
    "            inputs=trans_conv4_out,\n",
    "            filters=3,\n",
    "            kernel_size=[5, 5],\n",
    "            strides=[1, 1],\n",
    "            padding=\"SAME\",\n",
    "            kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02),\n",
    "            name=\"logits\",\n",
    "        )\n",
    "\n",
    "        out = tf.tanh(logits, name=\"out\")\n",
    "\n",
    "        return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FfguoTo4DdEr"
   },
   "outputs": [],
   "source": [
    "def discriminator(x, is_reuse=False, alpha=0.2):\n",
    "    \"\"\"Build the discriminator network.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    x : Input tensor for the discriminator\n",
    "    n_units: Number of units in hidden layer\n",
    "    reuse : Reuse the variables with tf.variable_scope\n",
    "    alpha : leak parameter for leaky ReLU\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    out, logits:\n",
    "    \"\"\"\n",
    "    with tf.compat.v1.variable_scope(\"discriminator\", reuse=is_reuse):\n",
    "        # Input layer 128*128*3 --> 64x64x64\n",
    "        # Conv --> BatchNorm --> LeakyReLU\n",
    "        conv1 = tf.compat.v1.layers.conv2d(\n",
    "            inputs=x,\n",
    "            filters=64,\n",
    "            kernel_size=[5, 5],\n",
    "            strides=[2, 2],\n",
    "            padding=\"SAME\",\n",
    "            kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02),\n",
    "            name=\"conv1\",\n",
    "        )\n",
    "\n",
    "        batch_norm1 = tf.compat.v1.layers.batch_normalization(\n",
    "            conv1, training=True, epsilon=1e-5, name=\"batch_norm1\"\n",
    "        )\n",
    "\n",
    "        conv1_out = tf.nn.leaky_relu(batch_norm1, alpha=alpha, name=\"conv1_out\")\n",
    "\n",
    "        # 64x64x64--> 32x32x128\n",
    "        # Conv --> BatchNorm --> LeakyReLU\n",
    "        conv2 = tf.compat.v1.layers.conv2d(\n",
    "            inputs=conv1_out,\n",
    "            filters=128,\n",
    "            kernel_size=[5, 5],\n",
    "            strides=[2, 2],\n",
    "            padding=\"SAME\",\n",
    "            kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02),\n",
    "            name=\"conv2\",\n",
    "        )\n",
    "\n",
    "        batch_norm2 = tf.compat.v1.layers.batch_normalization(\n",
    "            conv2, training=True, epsilon=1e-5, name=\"batch_norm2\"\n",
    "        )\n",
    "\n",
    "        conv2_out = tf.nn.leaky_relu(batch_norm2, alpha=alpha, name=\"conv2_out\")\n",
    "\n",
    "        # 32x32x128 --> 16x16x256\n",
    "        # Conv --> BatchNorm --> LeakyReLU\n",
    "        conv3 = tf.compat.v1.layers.conv2d(\n",
    "            inputs=conv2_out,\n",
    "            filters=256,\n",
    "            kernel_size=[5, 5],\n",
    "            strides=[2, 2],\n",
    "            padding=\"SAME\",\n",
    "            kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02),\n",
    "            name=\"conv3\",\n",
    "        )\n",
    "\n",
    "        batch_norm3 = tf.compat.v1.layers.batch_normalization(\n",
    "            conv3, training=True, epsilon=1e-5, name=\"batch_norm3\"\n",
    "        )\n",
    "\n",
    "        conv3_out = tf.nn.leaky_relu(batch_norm3, alpha=alpha, name=\"conv3_out\")\n",
    "\n",
    "        # 16x16x256 --> 16x16x512\n",
    "        # Conv --> BatchNorm --> LeakyReLU\n",
    "        conv4 = tf.compat.v1.layers.conv2d(\n",
    "            inputs=conv3_out,\n",
    "            filters=512,\n",
    "            kernel_size=[5, 5],\n",
    "            strides=[1, 1],\n",
    "            padding=\"SAME\",\n",
    "            kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02),\n",
    "            name=\"conv4\",\n",
    "        )\n",
    "\n",
    "        batch_norm4 = tf.compat.v1.layers.batch_normalization(\n",
    "            conv4, training=True, epsilon=1e-5, name=\"batch_norm4\"\n",
    "        )\n",
    "\n",
    "        conv4_out = tf.nn.leaky_relu(batch_norm4, alpha=alpha, name=\"conv4_out\")\n",
    "\n",
    "        # 16x16x512 --> 8x8x1024\n",
    "        # Conv --> BatchNorm --> LeakyReLU\n",
    "        conv5 = tf.compat.v1.layers.conv2d(\n",
    "            inputs=conv4_out,\n",
    "            filters=1024,\n",
    "            kernel_size=[5, 5],\n",
    "            strides=[2, 2],\n",
    "            padding=\"SAME\",\n",
    "            kernel_initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02),\n",
    "            name=\"conv5\",\n",
    "        )\n",
    "\n",
    "        batch_norm5 = tf.compat.v1.layers.batch_normalization(\n",
    "            conv5, training=True, epsilon=1e-5, name=\"batch_norm5\"\n",
    "        )\n",
    "\n",
    "        conv5_out = tf.nn.leaky_relu(batch_norm5, alpha=alpha, name=\"conv5_out\")\n",
    "\n",
    "        # Flatten it\n",
    "        flatten = tf.reshape(conv5_out, (-1, 8 * 8 * 1024))\n",
    "\n",
    "        # Logits\n",
    "        logits = tf.compat.v1.layers.dense(inputs=flatten, units=1, activation=None)\n",
    "\n",
    "        out = tf.sigmoid(logits)\n",
    "\n",
    "        return out, logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Loss\n",
    "\n",
    "The loss tells us how well the GAN is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F6M04hCmDjdq"
   },
   "outputs": [],
   "source": [
    "def model_loss(input_real, input_z, output_channel_dim, alpha):\n",
    "    \"\"\"\n",
    "    Get the loss for the discriminator and generator\n",
    "    :param input_real: Images from the real dataset\n",
    "    :param input_z: Z input\n",
    "    :param out_channel_dim: The number of channels in the output image\n",
    "    :return: A tuple of (discriminator loss, generator loss)\n",
    "    \"\"\"\n",
    "    # Generator network here\n",
    "    g_model = generator(input_z, output_channel_dim)\n",
    "    # g_model is the generator output\n",
    "\n",
    "    # Discriminator network here\n",
    "    d_model_real, d_logits_real = discriminator(input_real, alpha=alpha)\n",
    "    d_model_fake, d_logits_fake = discriminator(g_model, is_reuse=True, alpha=alpha)\n",
    "\n",
    "    # Calculate losses\n",
    "    d_loss_real = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=d_logits_real, labels=tf.ones_like(d_model_real)\n",
    "        )\n",
    "    )\n",
    "    d_loss_fake = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=d_logits_fake, labels=tf.zeros_like(d_model_fake)\n",
    "        )\n",
    "    )\n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "    g_loss = tf.reduce_mean(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits=d_logits_fake, labels=tf.ones_like(d_model_fake)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return d_loss, g_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZEfEYlYCDvEL"
   },
   "outputs": [],
   "source": [
    "def model_optimizers(d_loss, g_loss, lr_D, lr_G, beta1):\n",
    "    \"\"\"\n",
    "    Get optimization operations\n",
    "    :param d_loss: Discriminator loss Tensor\n",
    "    :param g_loss: Generator loss Tensor\n",
    "    :param learning_rate: Learning Rate Placeholder\n",
    "    :param beta1: The exponential decay rate for the 1st moment in the optimizer\n",
    "    :return: A tuple of (discriminator training operation, generator training operation)\n",
    "    \"\"\"\n",
    "    # Get the trainable_variables, split into G and D parts\n",
    "    t_vars = tf.compat.v1.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith(\"generator\")]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith(\"discriminator\")]\n",
    "\n",
    "    update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "    # Generator update\n",
    "    gen_updates = [op for op in update_ops if op.name.startswith(\"generator\")]\n",
    "\n",
    "    # Optimizers\n",
    "    with tf.control_dependencies(gen_updates):\n",
    "        d_train_opt = tf.compat.v1.train.AdamOptimizer(\n",
    "            learning_rate=lr_D, beta1=beta1\n",
    "        ).minimize(d_loss, var_list=d_vars)\n",
    "        g_train_opt = tf.compat.v1.train.AdamOptimizer(\n",
    "            learning_rate=lr_G, beta1=beta1\n",
    "        ).minimize(g_loss, var_list=g_vars)\n",
    "\n",
    "    return d_train_opt, g_train_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will show an image of our fake data while the GAN is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QblwpY0DDx-F"
   },
   "outputs": [],
   "source": [
    "def show_generator_output(\n",
    "    sess, n_images, input_z, out_channel_dim, image_mode, image_path, save, show\n",
    "):\n",
    "    \"\"\"\n",
    "    Show example output for the generator\n",
    "    :param sess: TensorFlow session\n",
    "    :param n_images: Number of Images to display\n",
    "    :param input_z: Input Z Tensor\n",
    "    :param out_channel_dim: The number of channels in the output image\n",
    "    :param image_mode: The mode to use for images (\"RGB\" or \"L\")\n",
    "    :param image_path: Path to save the image\n",
    "    \"\"\"\n",
    "    cmap = None if image_mode == \"RGB\" else \"gray\"\n",
    "    z_dim = input_z.get_shape().as_list()[-1]\n",
    "    example_z = np.random.uniform(-1, 1, size=[n_images, z_dim])\n",
    "\n",
    "    samples = sess.run(\n",
    "        generator(input_z, out_channel_dim, False), feed_dict={input_z: example_z}\n",
    "    )\n",
    "\n",
    "    images_grid = images_square_grid(samples, image_mode)\n",
    "\n",
    "    if save:\n",
    "        # Save image\n",
    "        images_grid.save(image_path, \"JPEG\")\n",
    "\n",
    "    if show:\n",
    "        display.clear_output(wait=True)\n",
    "        plt.imshow(images_grid, cmap=cmap)\n",
    "        display.display(plt.gcf())\n",
    "        # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vYgF4FEvD11n"
   },
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZPQLMoXoEHHI"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    epoch_count,\n",
    "    batch_size,\n",
    "    z_dim,\n",
    "    learning_rate_D,\n",
    "    learning_rate_G,\n",
    "    beta1,\n",
    "    get_batches,\n",
    "    data_shape,\n",
    "    data_image_mode,\n",
    "    alpha,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the GAN\n",
    "    :param epoch_count: Number of epochs\n",
    "    :param batch_size: Batch Size\n",
    "    :param z_dim: Z dimension\n",
    "    :param learning_rate: Learning Rate\n",
    "    :param beta1: The exponential decay rate for the 1st moment in the optimizer\n",
    "    :param get_batches: Function to get batches\n",
    "    :param data_shape: Shape of the data\n",
    "    :param data_image_mode: The image mode to use for images (\"RGB\" or \"L\")\n",
    "    \"\"\"\n",
    "    # Create our input placeholders\n",
    "    input_images, input_z, lr_G, lr_D = model_inputs(data_shape[1:], z_dim)\n",
    "\n",
    "    # Losses\n",
    "    d_loss, g_loss = model_loss(input_images, input_z, data_shape[3], alpha)\n",
    "\n",
    "    # Optimizers\n",
    "    d_opt, g_opt = model_optimizers(d_loss, g_loss, lr_D, lr_G, beta1)\n",
    "\n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "\n",
    "    with tf.compat.v1.Session(config=config) as sess:\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "        # Saver\n",
    "        saver = tf.compat.v1.train.Saver()\n",
    "\n",
    "        num_epoch = 0\n",
    "\n",
    "        if not os.path.isdir(image_folder):\n",
    "            os.mkdir(image_folder)\n",
    "\n",
    "        if not os.path.isdir(models_folder):\n",
    "            os.mkdir(models_folder)\n",
    "        model_save_path = os.path.join(models_folder, \"model.cpkt\")\n",
    "\n",
    "        if from_checkpoint:\n",
    "            saver.restore(sess, model_save_path)\n",
    "            image_path = \"new_train/new_gen_image.jpg\"\n",
    "            show_generator_output(\n",
    "                sess,\n",
    "                1,\n",
    "                input_z,\n",
    "                data_shape[3],\n",
    "                data_image_mode,\n",
    "                image_path,\n",
    "                False,\n",
    "                True,\n",
    "            )\n",
    "\n",
    "        for epoch_i in range(epoch_count):\n",
    "            num_epoch += 1\n",
    "            if num_epoch % 5 == 0:\n",
    "                saver.save(sess, model_save_path)\n",
    "                print(\"Model saved\")\n",
    "\n",
    "            # saves model every 50 epochs\n",
    "            if epoch_i > 50 and epoch_i % 50 == 0:\n",
    "                saver.save(\n",
    "                    sess, model_save_path, global_step=epoch_i, write_meta_graph=False\n",
    "                )\n",
    "            for batch_images in get_batches(batch_size):\n",
    "                # Random noise\n",
    "                batch_z = np.random.uniform(-1, 1, size=(batch_size, z_dim))\n",
    "                # Run optimizers\n",
    "                _ = sess.run(\n",
    "                    d_opt,\n",
    "                    feed_dict={\n",
    "                        input_images: batch_images,\n",
    "                        input_z: batch_z,\n",
    "                        lr_D: learning_rate_D,\n",
    "                    },\n",
    "                )\n",
    "                _ = sess.run(\n",
    "                    g_opt,\n",
    "                    feed_dict={\n",
    "                        input_images: batch_images,\n",
    "                        input_z: batch_z,\n",
    "                        lr_G: learning_rate_G,\n",
    "                    },\n",
    "                )\n",
    "\n",
    "            # will calculate losses and generate an image for each epoch\n",
    "\n",
    "            train_loss_d = d_loss.eval({input_z: batch_z, input_images: batch_images})\n",
    "            train_loss_g = g_loss.eval({input_z: batch_z})\n",
    "            g_losses.append(train_loss_g)\n",
    "            d_losses.append(train_loss_d)\n",
    "            # Save it\n",
    "            image_name = str(epoch_i) + \".jpg\"\n",
    "            image_path = os.path.join(image_folder, image_name)\n",
    "\n",
    "            plt.title(f\"Epoch {epoch_i + 1}\")\n",
    "            show_generator_output(\n",
    "                sess, 9, input_z, data_shape[3], data_image_mode, image_path, True, True\n",
    "            )\n",
    "            print(\n",
    "                \"Epoch {}/{} |\".format(epoch_i + 1, epoch_count),\n",
    "                \"Discriminator Loss: {:.4f} |\".format(train_loss_d),\n",
    "                \"Generator Loss: {:.4f}\".format(train_loss_g),\n",
    "            )\n",
    "\n",
    "    return d_losses, g_losses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9LQBDqPzEkH5"
   },
   "outputs": [],
   "source": [
    "# Size input image for discriminator\n",
    "real_size = (128, 128, 3)\n",
    "\n",
    "# Size of latent vector to generator\n",
    "z_dim = 100\n",
    "learning_rate_D = 0.000005  # Thanks to Alexia Jolicoeur Martineau https://ajolicoeur.wordpress.com/cats/\n",
    "learning_rate_G = 0.00002  # Thanks to Alexia Jolicoeur Martineau https://ajolicoeur.wordpress.com/cats/\n",
    "batch_size = 32\n",
    "epochs = 2000\n",
    "alpha = 0.2\n",
    "beta1 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n2gbQewtEoWP"
   },
   "outputs": [],
   "source": [
    "# Load the data and train the network here\n",
    "dataset = Dataset(resized_data_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iMkIdSOeEs3B",
    "outputId": "b696e715-e253-4e2b-c191-1ef2c66fa03a"
   },
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xtqOrcWuEwC6",
    "outputId": "029c43f7-6d9a-441f-84ea-be0cfd4850db"
   },
   "outputs": [],
   "source": [
    "with tf.Graph().as_default():\n",
    "    d_losses, g_losses = train(\n",
    "        epochs,\n",
    "        batch_size,\n",
    "        z_dim,\n",
    "        learning_rate_D,\n",
    "        learning_rate_G,\n",
    "        beta1,\n",
    "        dataset.get_batches,\n",
    "        dataset.shape,\n",
    "        dataset.image_mode,\n",
    "        alpha,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "d_losses = np.array(d_losses)\n",
    "g_losses = np.array(g_losses)\n",
    "plt.plot(d_losses, label=\"Discriminator\", alpha=0.5)\n",
    "plt.plot(g_losses, label=\"Generator\", alpha=0.5)\n",
    "plt.title(\"Training Losses\")\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the Generated Images\n",
    "\n",
    "- Give it a larger dataset (~10K images)\n",
    "- Run for a larger number of epochs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
